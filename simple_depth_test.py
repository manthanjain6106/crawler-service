#!/usr/bin/env python3
"""
Simple test script to verify depth handling improvements
"""

import asyncio
import sys
from crawler_service import WebCrawler
from models import CrawlRequest


async def test_basic_crawling():
    """Test basic crawling functionality"""
    print("ğŸ§ª Testing basic crawling functionality...")
    
    try:
        # Test with a simple URL
        request = CrawlRequest(
            url="https://httpbin.org/html",
            max_depth=1,
            follow_links=False,
            extract_text=True,
            extract_links=True
        )
        
        async with WebCrawler() as crawler:
            result = await crawler.crawl_website(request)
        
        print(f"âœ… Status: {result.status}")
        print(f"âœ… Total pages: {result.total_pages}")
        print(f"âœ… Duration: {result.duration:.2f}s")
        
        if result.pages:
            page = result.pages[0]
            print(f"âœ… First page URL: {page.url}")
            print(f"âœ… First page depth: {page.depth}")
            print(f"âœ… First page title: {page.title}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


async def test_depth_tracking():
    """Test depth tracking functionality"""
    print("\nğŸ§ª Testing depth tracking...")
    
    try:
        # Test with a URL that has links
        request = CrawlRequest(
            url="https://httpbin.org/links/2/0",
            max_depth=2,
            follow_links=True,
            extract_text=False,
            extract_links=True
        )
        
        async with WebCrawler() as crawler:
            result = await crawler.crawl_website(request)
        
        print(f"âœ… Status: {result.status}")
        print(f"âœ… Total pages: {result.total_pages}")
        
        # Check depth tracking
        depths = [page.depth for page in result.pages]
        print(f"âœ… Depths found: {sorted(set(depths))}")
        
        for page in result.pages:
            print(f"  Depth {page.depth}: {page.url}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


async def test_unlimited_depth():
    """Test unlimited depth functionality"""
    print("\nğŸ§ª Testing unlimited depth...")
    
    try:
        request = CrawlRequest(
            url="https://httpbin.org/links/1/0",
            max_depth=0,  # Unlimited depth
            follow_links=True,
            extract_text=False,
            extract_links=True
        )
        
        async with WebCrawler() as crawler:
            result = await crawler.crawl_website(request)
        
        print(f"âœ… Status: {result.status}")
        print(f"âœ… Total pages: {result.total_pages}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


async def main():
    """Run all tests"""
    print("ğŸš€ Starting depth handling tests...\n")
    
    tests = [
        test_basic_crawling,
        test_depth_tracking,
        test_unlimited_depth
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if await test():
                passed += 1
        except Exception as e:
            print(f"âŒ Test failed with exception: {e}")
    
    print(f"\nğŸ“Š Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Depth handling is working correctly.")
        return True
    else:
        print("âš ï¸  Some tests failed. Check the output above for details.")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Tests interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nğŸ’¥ Tests failed with error: {e}")
        sys.exit(1)
